{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b4cb059f",
   "metadata": {},
   "source": [
    "This notebook serves to reproduce all visualizations used in the paper. To run this notebook, first execute `main.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cdf2637",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open all required libraries and format for paper\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import matplotlib as mpl\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "from sklearn.metrics import log_loss, roc_auc_score\n",
    "import statsmodels.formula.api as smf\n",
    "\n",
    "import yaml\n",
    "\n",
    "def load_config(config_path):\n",
    "    with open(config_path, 'r') as f:\n",
    "        return yaml.safe_load(f)\n",
    "    \n",
    "sns.set(style=\"whitegrid\")\n",
    "mpl.rcParams.update({\n",
    "    'axes.titlesize': 35,\n",
    "    'axes.labelsize': 35,\n",
    "    'xtick.labelsize': 35,\n",
    "    'ytick.labelsize': 35,\n",
    "    'legend.fontsize': 35,\n",
    "    'pdf.fonttype': 42,  # For vector text in PDFs\n",
    "})\n",
    "# Set colorblind-friendly palette\n",
    "sns.set_palette(\"colorblind\")\n",
    "\n",
    "# For notebook environment, directly set the configuration\n",
    "experiment = \"mimic\"\n",
    "config_path = \"./configs\"\n",
    "\n",
    "config = load_config(Path(config_path) / f\"{experiment}_readmission.yaml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f1ba243",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pickle\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "\n",
    "tasks = [\"long_los\", \"in_hospital_mortality\", \"readmission\"]\n",
    "\n",
    "models = [\n",
    "    'Qwen3-8B',\n",
    "    'Qwen3-14B',\n",
    "    'Qwen3-32B',\n",
    "    #\"gpt-5\",\n",
    "    \"gemma-3-4b-it\",\n",
    "    \"gemma-3-27b-it\",\n",
    "    \"gpt-oss-20b\",\n",
    "    \"gpt-oss-120b\",\n",
    "    \"Llama-3.1-8B-Instruct\",\n",
    "    \"Llama-3.1-70B-Instruct\",\n",
    "    \"Mistral-Small-3.1-24B-Instruct-2503\",\n",
    "    \"medgemma-27b-text-it\",\n",
    "]\n",
    "\n",
    "renaming_model = {\n",
    "    \"Qwen3-8B\": \"Qwen3\\n8B\",\n",
    "    \"Qwen3-14B\": \"Qwen3\\n14B\",\n",
    "    'Qwen3-32B': \"Qwen3\\n32B\",\n",
    "    \"gpt-oss-20b\": \"GPT-OSS\\n20B\",\n",
    "    \"gpt-oss-120b\": \"GPT-OSS\\n120B\",\n",
    "    \"gpt-5\": \"GPT-5\",\n",
    "    \"gemma-3-4b-it\": \"Gemma\\n4B\",\n",
    "    \"gemma-3-12b-it\": \"Gemma\\n12B\",\n",
    "    \"gemma-3-27b-it\": \"Gemma\\n27B\",\n",
    "    \"Llama-3.1-8B-Instruct\": \"Llama3.1\\n8B\",\n",
    "    \"Llama-3.1-70B-Instruct\": \"Llama3.1\\n70B\",\n",
    "    \"medgemma-27b-text-it\": \"MedGemma\\n27B\",\n",
    "    \"Mistral-Small-3.1-24B-Instruct-2503\": \"Mistral 3.1\\n24B\",\n",
    "}\n",
    "\n",
    "# Each variant has a human-readable prefix (used in result keys) and flags\n",
    "variants = [\n",
    "    (\"Dropped\",              dict(exp_missing=False, prompt_missing=False)),\n",
    "    (\"Instruction\",   dict(exp_missing=False, prompt_missing=True)),\n",
    "    (\"Indicator\",            dict(exp_missing=True,  prompt_missing=False)),\n",
    "    (\"Indicator + Inst.\", dict(exp_missing=True,  prompt_missing=True)),\n",
    "]\n",
    "\n",
    "predictions_dir = Path(config[\"predictions_dir\"])\n",
    "eps = 1e-12\n",
    "\n",
    "def safe_load_pickle(path):\n",
    "    try:\n",
    "        with open(path, \"rb\") as f:\n",
    "            return pickle.load(f)\n",
    "    except:\n",
    "        print(path)\n",
    "        return None\n",
    "\n",
    "def file_name(task, model_name, exp_missing, labs_only, prompt_missing):\n",
    "    return predictions_dir / f\"predictions_{task}_{model_name}_{exp_missing}_{labs_only}_{prompt_missing}.pkl\"\n",
    "\n",
    "def is_prob(x):\n",
    "    return (x is not None and isinstance(x, (int, float)) and 0.0 <= float(x) <= 1.0)\n",
    "\n",
    "task_results_dicts = {}\n",
    "invalid_counts = {}\n",
    "\n",
    "for task in tasks:\n",
    "    config[\"downstream_task\"] = task\n",
    "    print(f\"Processing {task}...\")\n",
    "\n",
    "    # --- Load baseline (optional) ---\n",
    "    baseline = safe_load_pickle(predictions_dir / f\"baseline_predictions_{task}.pkl\")\n",
    "    if baseline is None:\n",
    "        print(f\"⚠️ No baseline predictions for {task}\")\n",
    "\n",
    "    # --- Load all model/variant dicts ---\n",
    "    # subject_dicts: {(model, variant_name): {sid: {...}}}\n",
    "    subject_dicts = {}\n",
    "    for m in models:\n",
    "        for vname, vflags in variants:\n",
    "            fname = file_name(task, m, vflags[\"exp_missing\"], config[\"labs_only\"], vflags[\"prompt_missing\"])\n",
    "            d = safe_load_pickle(fname)\n",
    "            if d is None:\n",
    "                print(f\"⚠️ No predictions for {task} | {m} | missing={vflags['exp_missing']} | prompt={vflags['prompt_missing']}\")\n",
    "                d = {}\n",
    "            subject_dicts[(m, vname)] = d\n",
    "\n",
    "    # --- Compute common subject IDs across all non-empty dicts (+ baseline if present) ---\n",
    "    nonempty_sets = [set(d.keys()) for d in subject_dicts.values() if d]\n",
    "    if not nonempty_sets:\n",
    "        print(f\"⚠️ No non-empty prediction dicts for {task}. Skipping.\")\n",
    "        task_results_dicts[task] = {}\n",
    "        continue\n",
    "\n",
    "    common_subject_ids = set.intersection(*nonempty_sets)\n",
    "    baseline_index_map = {}\n",
    "    if baseline is not None:\n",
    "        base_ids = list(baseline[\"lr_no_missing\"][\"subject_ids\"])\n",
    "        baseline_index_map = {sid: i for i, sid in enumerate(base_ids)}\n",
    "        common_subject_ids = common_subject_ids & set(baseline[\"lr_no_missing\"][\"subject_ids\"])\n",
    "\n",
    "    if not common_subject_ids:\n",
    "        print(f\"⚠️ No overlapping subject IDs for {task}. Skipping.\")\n",
    "        task_results_dicts[task] = {}\n",
    "        continue\n",
    "\n",
    "        # --- Pre-allocate aligned collectors ---\n",
    "    aligned_preds         = defaultdict(list)\n",
    "    aligned_serializations= defaultdict(list)\n",
    "    aligned_responses     = defaultdict(list)\n",
    "    aligned_loglosses     = defaultdict(list)\n",
    "\n",
    "    aligned_labels            = []  # single list reused across variants\n",
    "    aligned_baseline_preds_1  = []\n",
    "    aligned_baseline_preds_2  = []\n",
    "    aligned_missing_counts    = []\n",
    "    aligned_baseline_logloss_1= []\n",
    "    aligned_baseline_logloss_2= []\n",
    "    aligned_baseline_indices  = []\n",
    "\n",
    "    invalid_counts[task] = {(m, vname): 0 for m in models for vname, _ in variants}\n",
    "\n",
    "    # --- Iterate through subjects and collect aligned rows ---\n",
    "    for sid in common_subject_ids:\n",
    "        # label from any available dict (or baseline)\n",
    "        if baseline is not None and sid in baseline_index_map:\n",
    "            label = baseline[\"lr_no_missing\"][\"labels\"][baseline_index_map[sid]]\n",
    "        else:\n",
    "            # take from first dict that has it\n",
    "            label = None\n",
    "            for d in subject_dicts.values():\n",
    "                if sid in d:\n",
    "                    label = d[sid].get(\"label\", None)\n",
    "                    if label is not None:\n",
    "                        break\n",
    "        if label is None:\n",
    "            # cannot validate log loss or continue safely\n",
    "            continue\n",
    "\n",
    "        # validate predictions exist and are in [0,1] for ALL requested (model,variant)\n",
    "        preds_ok = True\n",
    "        for (m, vname), d in subject_dicts.items():\n",
    "            if sid not in d or \"prediction\" not in d[sid] or not is_prob(d[sid][\"prediction\"]):\n",
    "                invalid_counts[task][(m, vname)] += 1 \n",
    "                preds_ok = False\n",
    "                break\n",
    "        if not preds_ok:\n",
    "            continue\n",
    "\n",
    "        # collect per variant/model\n",
    "        for (m, vname), d in subject_dicts.items():\n",
    "            pred = float(d[sid][\"prediction\"])\n",
    "            serialization = d[sid].get(\"serialization\")\n",
    "            response      = d[sid].get(\"response\")\n",
    "\n",
    "            aligned_preds[(m, vname)].append(pred)\n",
    "            aligned_serializations[(m, vname)].append(serialization)\n",
    "            aligned_responses[(m, vname)].append(response)\n",
    "\n",
    "            ll = -(label * np.log(pred + eps) + (1 - label) * np.log(1 - pred + eps))\n",
    "            aligned_loglosses[(m, vname)].append(ll)\n",
    "\n",
    "        # baseline (optional)\n",
    "        if baseline is not None and sid in baseline_index_map:\n",
    "            b_idx  = baseline_index_map[sid]\n",
    "            b_pred1 = float(baseline[\"lr_no_missing\"][\"predictions\"][b_idx])\n",
    "            b_pred2 = float(baseline[\"lr_with_missing\"][\"predictions\"][b_idx])\n",
    "            m_count = baseline[\"lr_with_missing\"][\"missing_counts\"][b_idx]\n",
    "            b_label = baseline[\"lr_no_missing\"][\"labels\"][b_idx]\n",
    "\n",
    "            aligned_labels.append(b_label)\n",
    "            aligned_baseline_preds_1.append(b_pred1)\n",
    "            aligned_baseline_preds_2.append(b_pred2)\n",
    "            aligned_missing_counts.append(m_count)\n",
    "\n",
    "            b_ll1 = -(label * np.log(b_pred1 + eps) + (1 - label) * np.log(1 - b_pred1 + eps))\n",
    "            b_ll2 = -(label * np.log(b_pred2 + eps) + (1 - label) * np.log(1 - b_pred2 + eps))\n",
    "            aligned_baseline_logloss_1.append(b_ll1)\n",
    "            aligned_baseline_logloss_2.append(b_ll2)\n",
    "            aligned_baseline_indices.append(b_idx)\n",
    "\n",
    "    # --- Build the final task dict programmatically ---\n",
    "    task_bundle = {}\n",
    "\n",
    "    # add model/variant blocks\n",
    "    for m in models:\n",
    "        for vname, _ in variants:\n",
    "            task_bundle[(m, vname)] = {\n",
    "                \"predictions\": aligned_preds[(m, vname)],\n",
    "                \"labels\": aligned_labels,  # same aligned order\n",
    "                \"serializations\": aligned_serializations[(m, vname)],\n",
    "                \"responses\": aligned_responses[(m, vname)],\n",
    "                \"log_losses\": aligned_loglosses[(m, vname)],\n",
    "            }\n",
    "\n",
    "    # add baselines (if present)\n",
    "    if baseline is not None:\n",
    "        task_bundle[(\"Log\", \"Dropped\")] = {\n",
    "            \"predictions\": aligned_baseline_preds_1,\n",
    "            \"labels\": aligned_labels,\n",
    "            \"log_losses\": aligned_baseline_logloss_1,\n",
    "        }\n",
    "        task_bundle[(\"Log\", \"Indicator\")] = {\n",
    "            \"predictions\": aligned_baseline_preds_2,\n",
    "            \"labels\": aligned_labels,\n",
    "            \"missing_counts\": aligned_missing_counts,\n",
    "            \"log_losses\": aligned_baseline_logloss_2,\n",
    "            \"feature_importances\": baseline[\"lr_with_missing\"].get(\"feature_importances\") if baseline else None,\n",
    "            \"indices\": aligned_baseline_indices,\n",
    "        }\n",
    "\n",
    "    task_results_dicts[task] = task_bundle\n",
    "\n",
    "    print(f\"Num overlapping for task {task}: {len(aligned_labels)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c980c51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the number of errors per model\n",
    "\n",
    "def make_invalid_table_latex(\n",
    "    invalid_counts: dict,\n",
    "    variant_order: list[str] | None = None,\n",
    "    task_order: list[str] | None = None,\n",
    "    caption: str = \"Invalid prediction counts by task and variant.\",\n",
    "    label: str = \"tab:invalid_counts\",\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    invalid_counts: {task: {variant: count}}\n",
    "    Output: LaTeX table with tasks as columns, variants as rows (integers only).\n",
    "    \"\"\"\n",
    "    # Collect tasks/variants\n",
    "    tasks = list(invalid_counts.keys())\n",
    "    variants = set()\n",
    "    for t, d in invalid_counts.items():\n",
    "        variants.update(d.keys())\n",
    "    # tasks = task_order or sorted(tasks)\n",
    "    variants = variant_order or sorted(variants)\n",
    "\n",
    "    # Build wide table (variants x tasks), fill missing with 0 and cast to int\n",
    "    df = pd.DataFrame(\n",
    "        {t: {v: invalid_counts.get(t, {}).get(v, 0) for v in variants} for t in tasks}\n",
    "    ).fillna(0).astype(int)\n",
    "    df.index.name = \"Variant\"\n",
    "    return df.to_latex()\n",
    "\n",
    "\n",
    "methods = list(task_results_dicts[tasks[0]].keys())[0:-2]\n",
    "\n",
    "# Example call (no totals; integers only)\n",
    "latex = make_invalid_table_latex(\n",
    "    invalid_counts,\n",
    "    variant_order=methods,\n",
    "    caption=\"Invalid predictions per task/variant.\",\n",
    "    label=\"tab:invalid_counts\"\n",
    ")\n",
    "print(latex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2408ce27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute performance and calibration\n",
    "def bootstrap_performance(labels, predictions, n_bootstrap=1000, n_bins=5, ece_strategy='adaptive', pi=None, eps=1e-12):\n",
    "    \"\"\"Bootstrap calibration curve and ECE with confidence intervals\"\"\"\n",
    "    n_samples = len(labels)\n",
    "    ece_bootstrap = []\n",
    "    auroc_bootstrap = []\n",
    "    \n",
    "    labels = np.array(labels)\n",
    "    predictions = np.array(predictions)\n",
    "    \n",
    "    for _ in range(n_bootstrap):\n",
    "        # Bootstrap sample\n",
    "        indices = np.random.choice(n_samples, n_samples, replace=True)\n",
    "        boot_labels = np.array(labels)[indices]\n",
    "        boot_preds = np.array(predictions)[indices]\n",
    "\n",
    "        # Compute ECE for this bootstrap\n",
    "        if ece_strategy == 'quantile':\n",
    "            edges = np.quantile(boot_preds, np.linspace(0, 1, n_bins+1))\n",
    "            edges[0], edges[-1] = 0.0, 1.0\n",
    "        else:\n",
    "            edges = np.linspace(0, 1, n_bins+1)\n",
    "\n",
    "        if pi is None:\n",
    "            w = np.ones_like(boot_labels)\n",
    "        else:\n",
    "            w = np.where(boot_labels == 1, pi / 0.5, (1 - pi) / (1 - 0.5)).astype(float)\n",
    "\n",
    "        ece = 0.0\n",
    "        total_weight = w.sum()\n",
    "\n",
    "        for b in range(n_bins):\n",
    "            if b < n_bins - 1:\n",
    "                mask = (boot_preds >= edges[b]) & (boot_preds < edges[b+1])\n",
    "            else:\n",
    "                mask = (boot_preds >= edges[b]) & (boot_preds <= edges[b+1])\n",
    "            if not mask.any():\n",
    "                continue\n",
    "\n",
    "            pw = boot_preds[mask]\n",
    "            yw = boot_labels[mask]\n",
    "            ww = w[mask]\n",
    "\n",
    "            conf = np.average(pw, weights=ww)\n",
    "            acc  = np.average(yw, weights=ww)\n",
    "            ece += (ww.sum() / total_weight) * abs(acc - conf)\n",
    "        ece_bootstrap.append(ece)\n",
    "        auroc = roc_auc_score(boot_labels, boot_preds)\n",
    "        auroc_bootstrap.append(auroc)\n",
    "    \n",
    "    # Convert to arrays\n",
    "    ece_bootstrap = np.array(ece_bootstrap)\n",
    "    auroc_bootstrap = np.array(auroc_bootstrap)\n",
    "    \n",
    "    # ECE stats\n",
    "    ece_mean = np.mean(ece_bootstrap)\n",
    "    ece_lower = np.percentile(ece_bootstrap, 2.5)\n",
    "    ece_upper = np.percentile(ece_bootstrap, 97.5)\n",
    "\n",
    "    auroc_mean = np.mean(auroc_bootstrap)\n",
    "    auroc_lower = np.percentile(auroc_bootstrap, 2.5)\n",
    "    auroc_upper = np.percentile(auroc_bootstrap, 97.5)\n",
    "    \n",
    "    return {\n",
    "        \"ece\": {\n",
    "            \"mean\": ece_mean,\n",
    "            \"lower\": ece_lower,\n",
    "            \"upper\": ece_upper,\n",
    "        },\n",
    "        \"auroc\": {\n",
    "            \"mean\": auroc_mean,\n",
    "            \"lower\": auroc_lower,\n",
    "            \"upper\": auroc_upper,\n",
    "        },\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d775ae1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get bootstrapped perforamnces\n",
    "task_metrics = {}\n",
    "for task in tasks:\n",
    "    task_metrics[task] = {}\n",
    "\n",
    "    for key, d in task_results_dicts[task].items():\n",
    "        print(key)\n",
    "        if len(d[\"predictions\"]) > 0:\n",
    "            task_metrics[task][key] = bootstrap_performance(d[\"labels\"], d[\"predictions\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c9ac5ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create regression\n",
    "metric = 'auroc'\n",
    "\n",
    "for task in tasks:\n",
    "    np.random.seed(42)\n",
    "    # Create array\n",
    "    task_data = pd.DataFrame({model: task_metrics[task][model][metric] for model in task_metrics[task]})\n",
    "\n",
    "    # Difference performance\n",
    "    difference = task_data.sub(task_data.xs('Dropped', axis=1, level=1), level=0, axis=1)\n",
    "    difference = difference.loc[:, difference.columns.get_level_values(1) != 'Dropped']\n",
    "\n",
    "    # Extract model and size\n",
    "    pattern = r'^([^-]+).*?(\\d+[Bb])'\n",
    "    model_size = difference.columns.get_level_values(0).to_series().str.extract(pattern)\n",
    "    model_size.columns = ['Model', 'Size']\n",
    "    model_size['Size'] = model_size['Size'].str.replace(r'[Bb]', '', regex=True).fillna(300).astype(int).apply(np.log)\n",
    "    model_size['Model'] = model_size['Model'].fillna('gpt')\n",
    "    model_size['Prompt'] = difference.columns.get_level_values(1).to_series().str.contains('Inst').values\n",
    "    model_size['Serialization'] = difference.columns.get_level_values(1).to_series().str.contains('Ind').values\n",
    "\n",
    "    difference.columns = pd.MultiIndex.from_frame(model_size)\n",
    "\n",
    "    # Fitting a regression on this values\n",
    "    train = None\n",
    "    test = None\n",
    "\n",
    "    # Regressed on model size, token used for training, medical data used and \n",
    "    selection = ['mean', 'Size', 'Prompt', 'Serialization']\n",
    "    \n",
    "    model = smf.ols(formula='mean ~ Size * C(Prompt) + Size * C(Serialization)', data=difference.T.reset_index()[selection]).fit()\n",
    "    print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f23b8e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display x axis perf with and y without - size reflect model size and color model class\n",
    "metric = 'ece'\n",
    "for task in tasks:\n",
    "    for prompt in ['Indicator + Inst.', 'Indicator', 'Instruction']:\n",
    "        # Create array\n",
    "        task_data = pd.DataFrame({model: task_metrics[task][model][metric] for model in task_metrics[task] if model[0] != 'Log'})\n",
    "        model_size = task_data.columns.get_level_values(0).to_series().str.extract(pattern)\n",
    "        model_size.columns = ['Model', 'Size']\n",
    "        model_size['Size'] = model_size['Size'].str.replace(r'[Bb]', '', regex=True).fillna(300).astype(int)\n",
    "        model_size['Model'] = model_size['Model'].fillna('gpt')\n",
    "        model_size['Variant'] = task_data.columns.get_level_values(1)\n",
    "        task_data.columns = pd.MultiIndex.from_frame(model_size)\n",
    "        task_data = task_data.loc['mean']\n",
    "\n",
    "        # Plot the gain in ece given size\n",
    "        plt.figure(figsize=(8,6))\n",
    "\n",
    "        ax = plt.scatter(task_data.xs('Dropped', level=2), \n",
    "                         task_data.xs(prompt, level=2), \n",
    "                        s = 10 * (task_data.xs('Dropped', level=2).index.get_level_values('Size') + 10), \n",
    "                        c = task_data.xs('Dropped', level=2).index.get_level_values('Model').astype(\"category\").codes, \n",
    "                        cmap='viridis', alpha = 0.75)\n",
    "        \n",
    "        # LEGEND\n",
    "        ########\n",
    "        models = task_data.xs('Dropped', level=2).index.get_level_values('Model').astype(\"category\")\n",
    "        model_codes = models.codes\n",
    "        unique_models = models.categories\n",
    "        unique_codes = np.unique(model_codes)\n",
    "\n",
    "        cmap = plt.cm.viridis\n",
    "        import matplotlib.patches as mpatches\n",
    "        # Create one legend entry per model\n",
    "        matches = {\n",
    "            'gpt': 'GPT', 'medgemma': 'Med-Gemma', 'gemma':'Gemma'\n",
    "        }\n",
    "\n",
    "        handles = [\n",
    "            plt.scatter([], [], \n",
    "                color=cmap(code / (len(unique_codes)-1)),\n",
    "                s=200,              # legend marker size\n",
    "                marker='o',\n",
    "                label=matches[model] if model in matches else model)\n",
    "            for code, model in zip(unique_codes, unique_models)\n",
    "        ]\n",
    "\n",
    "        #plt.legend(handles=handles, loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "        #########\n",
    "\n",
    "        if task == \"long_los\":\n",
    "            task_name = \"Long LOS\"\n",
    "        elif task == \"death\" or task == \"in_hospital_mortality\":\n",
    "            task_name = \"Mortality\"\n",
    "        else:\n",
    "            task_name = task.capitalize()\n",
    "        plt.title(f\"{task_name}\")\n",
    "\n",
    "        plt.axvline(task_metrics[task][('Log', 'Dropped')][metric]['mean'], ls = '--', lw = 3, c = 'orange')\n",
    "        plt.axhline(task_metrics[task][('Log', 'Indicator')][metric]['mean'], ls = '--', lw = 3, c = 'orange')\n",
    "        \n",
    "        plt.axline((task_data.min(),task_data.min()), slope=1, ls = ':', c = 'k')\n",
    "\n",
    "        if metric == 'auroc':\n",
    "            if task == 'long_los':\n",
    "                plt.ylim(0.55, 0.8)\n",
    "                plt.xlim(0.55, 0.8)\n",
    "            elif task_name == 'Mortality':\n",
    "                plt.ylim(0.65, 0.9)\n",
    "                plt.xlim(0.65, 0.9)\n",
    "            else:\n",
    "                plt.ylim(0.5, 0.7)\n",
    "                plt.xlim(0.5, 0.7)\n",
    "        else:\n",
    "            plt.ylim(-0.05, 0.55)\n",
    "            plt.xlim(-0.05, 0.55)\n",
    "\n",
    "        plt.ylabel(metric.upper() + ' ' + prompt)\n",
    "        plt.xlabel(metric.upper() + ' Dropped')\n",
    "        plt.savefig('outputs/{}/{}_{}_{}.png'.format(experiment, prompt.replace(' + ', '_'), task, metric), bbox_inches='tight')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dfe4d47",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
